name: Multi-Account Deployment
on:
  push:
    branches:
      - main
  workflow_dispatch:
jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      id-token: write  # This is needed for OIDC auth with AWS
      contents: read
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python 3.8
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 urllib3 jq
      
      - name: Create tmp directory
        run: mkdir -p ./tmp
      
      - name: Configure AWS credentials for dev
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::470822489487:role/quicksightpoc-trustrole
          aws-region: us-east-1
          role-session-name: GitHubDeploymentSession
      
      - name: Build in dev environment
        run: python folderexport.py --account-id 470822489487 --region us-east-1 --folder-id f286a908-ed92-4b89-ad22-af4c2a0b54b3 --output ./tmp/QuickSightBundle.zip
      
      - name: Process QuickSight Bundle
        run: |
          # Create temporary directories for processing
          mkdir -p ./tmp/process_dir
          mkdir -p ./tmp/nested_process
          
          # Extract the main zip file
          unzip -q ./tmp/QuickSightBundle.zip -d ./tmp/process_dir
          
          echo "Analyzing QuickSight bundle structure..."
          find ./tmp/process_dir -type f -name "*.json" | sort
          
          # Create a script to fix the non-relational data source issue
          cat > ./tmp/fix_datasource.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import os
          import sys
          import re

          def fix_dataset_json(file_path):
              print(f"Processing dataset file: {file_path}")
              
              with open(file_path, 'r') as f:
                  try:
                      data = json.load(f)
                  except json.JSONDecodeError as e:
                      print(f"Error decoding JSON: {e}")
                      return False
              
              modified = False
              
              # Check if this is a dataset definition with customSql
              if 'DataSetConfiguration' in data:
                  config = data['DataSetConfiguration']
                  
                  # Check if using a non-relational data source with CustomSQL
                  if 'PhysicalTableMap' in config:
                      for table_id, table_def in config['PhysicalTableMap'].items():
                          if 'CustomSql' in table_def:
                              custom_sql = table_def['CustomSql']
                              if 'DataSourceArn' in custom_sql:
                                  # This checks for the problematic data source ID
                                  if '1e39287a-aafe-48a8-a8d1-7d8e25a93c4f' in custom_sql['DataSourceArn']:
                                      print(f"Found non-relational data source in CustomSQL")
                                      
                                      # Replace with a known relational data source ID
                                      # For this fix, we'll use a different data source ID 
                                      # that should be compatible with SQL
                                      new_ds_id = '221553ff-d80a-4861-8890-ae7e028016b7'
                                      custom_sql['DataSourceArn'] = custom_sql['DataSourceArn'].replace(
                                          '1e39287a-aafe-48a8-a8d1-7d8e25a93c4f', 
                                          new_ds_id
                                      )
                                      modified = True
                                      print(f"Replaced data source ID in CustomSQL")
              
              if modified:
                  # Write the modified data back to the file
                  with open(file_path, 'w') as f:
                      json.dump(data, f, indent=2)
                  print(f"Updated {file_path}")
                  return True
              
              return False

          def process_directory(directory):
              print(f"Processing directory: {directory}")
              modified_files = 0
              
              # Find all JSON files
              for root, dirs, files in os.walk(directory):
                  for file in files:
                      if file.endswith('.json'):
                          file_path = os.path.join(root, file)
                          if fix_dataset_json(file_path):
                              modified_files += 1
              
              return modified_files

          if __name__ == "__main__":
              if len(sys.argv) != 2:
                  print("Usage: fix_datasource.py <directory>")
                  sys.exit(1)
                  
              directory = sys.argv[1]
              modified = process_directory(directory)
              print(f"Fixed {modified} files")
          EOF
          
          # Make the script executable
          chmod +x ./tmp/fix_datasource.py
          
          # Execute the fix script
          python ./tmp/fix_datasource.py ./tmp/process_dir
          
          # Function to recursively process all zip files and JSON files
          process_directory() {
            local dir=$1
            
            # Process all text files (including JSON) in this directory
            find "$dir" -type f | while read file; do
              # Check if it's a text file using the file command
              if file "$file" | grep -q "text"; then
                sed -i \
                  -e 's/347f0c50-33a1-4fd2-bdd5-6f8867a05601/1e39287a-aafe-48a8-a81d-7d8e25a93c4f/g' \
                  -e 's/3519323f-3db4-4585-a0c1-a1df2698e3e0/221553ff-d80a-4861-8890-ae7e028016b7/g' \
                  -e 's/bb9c4023-1f25-472b-bf31-7a8ded7c2c69/78955b0d0-ee47-4915-aac4-c4f9dffa6821/g' \
                  -e 's/7d86f2f9-5bdf-402d-8d47-d7ab76bbaf87/52825973-e237-4adb-8fe8-015c046066b4/g' \
                  -e 's/-dev-/-tst-/g' \
                  "$file"
              fi
            done
            
            # Find and process all nested zip files
            find "$dir" -name "*.zip" -type f | while read zip_file; do
              echo "Processing nested zip file: $zip_file"
              
              # Create a unique directory for this nested zip
              nested_dir="./tmp/nested_process/$(basename "$zip_file" .zip)"
              mkdir -p "$nested_dir"
              
              # Extract the nested zip
              unzip -q "$zip_file" -d "$nested_dir"
              
              # Process the extracted contents recursively
              process_directory "$nested_dir"
              
              # Re-zip the processed contents
              original_dir=$(pwd)
              cd "$nested_dir"
              rm -f "$zip_file"  # Remove the original zip file
              zip -q -r "$zip_file" *  # Create new zip with processed contents
              cd "$original_dir"
              
              # Replace the original zip file with the processed one
              mv "$nested_dir/$zip_file" "$zip_file"
              
              # Clean up the temporary nested directory
              rm -rf "$nested_dir"
            done
          }
          
          # Start processing from the root extraction directory
          process_directory "./tmp/process_dir"
          
          # Re-create the main zip file with all processed content
          cd ./tmp/process_dir
          rm -f ../QuickSightBundle.zip  # Remove the original zip file
          zip -q -r ../QuickSightBundle.zip *  # Create new zip with all processed contents
          cd ../../
          
          # Cleanup temporary directories
          rm -rf ./tmp/process_dir ./tmp/nested_process
      
      - name: Upload QuickSight bundle artifact
        uses: actions/upload-artifact@v4
        with:
          name: quicksight-bundle
          path: ./tmp/QuickSightBundle.zip
          retention-days: 5
  import-qa:
    needs: build
    runs-on: ubuntu-latest
    permissions:
      contents: read  # No OIDC permissions needed for this job
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python 3.8
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 urllib3 awscli
      
      - name: Create tmp directory
        run: mkdir -p ./tmp
      
      - name: Download QuickSight bundle artifact
        uses: actions/download-artifact@v4
        with:
          name: quicksight-bundle
          path: ./tmp
      # Using AWS access keys from secrets for QA
      - name: Configure AWS credentials for QA using access keys
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.QA_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.QA_AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      
      - name: Debug AWS identity
        run: aws sts get-caller-identity
      
      - name: Import in QA environment
        run: python folderimport.py --file ./tmp/QuickSightBundle.zip --account-id 269801428807 --region us-east-1
